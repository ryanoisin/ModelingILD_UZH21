
---
title: "Modeling ILD - Discrete Time Models"
author: "Ois√≠n Ryan with thanks to Noemi Schuurman"
date: "November 2021"
params:
  answers: true
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    df_print: paged
    theme: paper
  pdf_document:
    toc: true
    toc_depth: 5
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this practical you will practice with fitting (Bayesian) single-subject and multi-level time series models, specifically the AR(1), AR(2) and VAR(1) models.

## Loading packages and data

The analyses in this lab depend on two `R`-packages:  [rjags](https://cran.r-project.org/web/packages/rjags/index.html), for fitting the Bayesian models;  [coda](https://cran.r-project.org/web/packages/coda/index.html), for running summary stats and plots on the Bayesian analysis results. In order for these analyses to run on your computer, you will also need to download and install [JAGS](https://sourceforge.net/projects/mcmc-jags/), because rjags depends on this program.

```{r Results='hide', message=FALSE, warning = FALSE}
if (!require("rjags")) install.packages("rjags")
if (!require("coda")) install.packages("coda")
if (!require("vars")) install.packages("vars")

library(rjags)
library(coda)

```

We will work with the two .Rds files for two female participants Jane and Mary (Names ficTional, data courtesy of Schuurman, Houtveen and Hamaker, 2015). The data consists of daily diary measurements, for 91 days for Jane and 105 days for Mary. The data contains various measures on mood,  positive and negative affect and symptoms surrounding the women's periods. Load the data with function `readRDS()`:

```{r, warning=FALSE}
mydata_Jane= readRDS("data_Jane.Rds")  

mydata_Mary= readRDS("data_Mary.Rds")  
```

Note that the variable names in the files are in Dutch, but we will translate the relevant variables on the go when we use them. 

We will start by looking at both women's rating (scale 1-100 low-high) of their daily overall mood.
```{r}
# Extracting mood from Jane's dataset 
dat_Jane = as.matrix(mydata_Jane$scale)
colnames(dat_Jane) = c("mood")

# Extracting mood from Mary's dataset
dat_Mary = as.matrix(mydata_Mary$scale)
colnames(dat_Mary) = c("mood")
```

# Ex.1: Exploring the data
From now on the code we have prepared is code for Jane, but not for Mary. Try to adjust the code for Mary yourself. 

We start by making a time series plot for the mood of Jane.

```{r}
# Visualizing the data of Jane: Time series plot #
ts.plot(dat_Jane,col="mediumseagreen",lwd=4, ylab= "Mood")

```

It doesn't look like there's any noticeable trend here, since it looks like mood varies pretty much around a value close to 75 for the duration of the observation window. We can test for trends explicitly, but let's move on for now, and inspect some basic descriptives, such as the mean and variation of mood over time, and the number of missing values. 

```{r}
# Exploring the data of Jane:
mean(dat_Jane, na.rm=TRUE)
var(dat_Jane, na.rm=TRUE)
sd(dat_Jane, na.rm=TRUE)
sum(is.na(dat_Jane))

```

Now we inspect the autocorrelations and partial autocorrelations of the  data using the functions  `acf()`, and `pacf()`.
```{r}
### ACF and PACF Jane:
acf(dat_Jane,na.action=na.pass)
pacf(dat_Jane,na.action=na.pass)

```

Based on the autocorrelations and partial autocorrelations for the different lags of the women's data, what order of autoregressive model would you fit for each of them (how many lags would you like to include)?

# Ex.2: Basic AR models
Now that we've explored the data a little, let's see how we can fit time series models to the data.

## Simple AR(1) Model in R with lm()
The simplest way to fit a n=1 time series model is estimating it as a standard frequentist regression model, for example using the `lm()` function. For this we need to first create a lagged predictor variable. For now, we will use now only a lag-1 model.

```{r}
## A function to lag the data ##
lagmatrix <- function(x,max.lag) embed(c(rep(NA,max.lag), x), max.lag+1)

## Lag the data of Jane ##
y1L_Jane = lagmatrix(dat_Jane, 1)[, 2] 

head(cbind(dat_Jane,y1L_Jane) )
```

We can now simply estimate the linear regression model:

```{r}
ar1lm_Jane <- lm(dat_Jane~y1L_Jane) 
summary(ar1lm_Jane)
# Note the remark in the output on the (listwise) deletion of missing observations!

plot(cbind(dat_Jane,y1L_Jane),xlab ="Mood",ylab="Mood_lag1" ) # Visualize the autocorrelation.

```


## Simple AR Models in R with arima()
We can easily fit n=1 time series models with package `arima` which is native in R.

ARIMA uses a state space modeling framework which combines maximum likelihood estimation and the kalman filter by default. Use ?arima in R to learn more. It hence differs from the OLS approach above with `lm()`, for example because missing data is simply deleted in the OLS case. An important advantage of the arima (and also the Bayesian) approach over the `lm()` approach in the previous exercise, is that we do not supplement the lagged variable as an exogenous variable (which we had to make ourselves). The lagging is taken care of by how we specify the model and estimate it (this will be more obvious in the Bayesian exercise). This also makes it easier to fit all sorts of different time series models with an easy specification.

Below, we first estimate an AR(1) model for Jane

```{r}
ar1arima <- arima(dat_Jane, c(1,0,0))
ar1arima
```

Compare the results to the OLS case - are the autoregression coefficients similar?

Let's fit an AR(2) model using the below syntax
````{r}
ar2arima <- arima(dat_Jane, c(2,0,0))
ar2arima
```


Which of these two models should we prefer for Jane? One helpful thing we can use to make this decision is to plot the ACFs of the **residuals** for each model. Let's compare these below, what do you see?

```{r}
acf(ar1arima$residuals, na.action = na.pass, main = "ACF AR(1) residuals")
acf(ar2arima$residuals, na.action = na.pass, main = "ACF AR(2) residuals")

```

You can also use *information criteria* to choose between these models. Notice that the arima function gives you an AIC for each model (smaller values indicating a better model). Of course, you can also make this choice based on your interpretation of the model and the resulting coefficients.

# Ex.3:  Multivariate Time-Series Models

Next we will fit a multivariate, Vector Autoregressive model (of order 1); a VAR(1) model. 
We will fit the model for two variables, overall mood like before, and nervous tension. 

Start by loading the data and take a look at their time series data. Do the variables seem to sync up over time?
What kind of effects would you expect nervous tension and mood to have on each other over time? What about concurrently?

```{r}
datvar_Jane = data.frame(mydata_Jane$scale,
                         mydata_Jane$nerveuzegespannenheid)
colnames(datvar_Jane) = c("mood", "nervoustension")

mood_J = datvar_Jane[,1]
tense_J = datvar_Jane[,2]
ts.plot(
  mood_J / 10,
  col = "mediumseagreen",
  lwd = 3,
  ylab = "scores",
  ylim = c(1, 10)
)
lines(tense_J, col = "royalblue3", lwd = 3)
```

We can also use the `acf()` and `pacf()` functions we used previously. When we have a multivariate time series like this, we can also see how lagged versions of mood and nervous tension depend on each other over time, using the *cross-correlation function*, in the function `ccf()`. Below we can see that there is some strong same-time-point correlation (lag = 0), and a modeeratly strong lag 1 cross-correlation.

```{r}
ccf(mood_J, tense_J, na.action = na.pass)
```


## VAR(1) model with OLS

Just as we did for the AR(p) models, we can fit the VAR(1) model using OLS. But again, we will run into problems with this approach if we have missing data. For example, if we try to use the popular `vars` package for this, we will simply get an error if there are missing values. To use OLS we must again apply listwise deletion, then estimate the VAR model "equation by equation", i.e., first estimating the model for mood, then for tension.

While I don't recommend it, let's do some listwise deletion so we can understand the output of the `vars` package.

```{r}
library(vars)
# we set p = 1 to ask for a VAR(1) model
vdata <- cbind(mood_J, tense_J)
# remove missings
miss <- apply(vdata, 1, function(row) any(is.na(row)))
vdata <- vdata[!miss,]

# now we can use the VAR function
var1_out <- VAR(vdata, p = 1)
summary(var1_out)
```

The output here tells us that there is a significant lagged effect of tension on both itself and on mood. 

Of course, we may also be interested in performing some model selection. We could again use an approach based on inspecting the residuals, but for larger models this may quickly become impractical. An alternative would be to use information criteria, such as the AIC, as we did above. The `vars` package function `VARselect()` allows you to test a variety of different VAR(p) models, and computes a number of different information criteria

```{r}
VARselect(vdata)
```

Here we see that a VAR(1) model is selected by all criteria. 


# Ex.4: Bayesian (V)AR models using JAGS
In this exercise we will examine how we can fit single-subject AR(p) and VAR(p) models in Bayesian software, using the Bayesian software JAGs in combination with the R package Rjags. We'll start by repeating the analyses in the previous exercises, to get a feel for how the Bayesian model specification works. We will use these same principles to specify multi-level models later on, so it's important you understand the basics first.


## AR(1) model using JAGS
In the file 'AR1model.txt' you will find the JAGS code for the model, which we will supply to jags for the analysis. It is also presented here below. Take a look at the model specification, including the priors that were specified. Do you understand what is being specified where?

Note: The priors specified are intended to be uninformative. 
Some notes on reading the code: 

* The ordering of the lines in JAGS codes does not matter (hence we can specify the function for y[t] before the function of muy[t] in the code below). 
* Normal distributions in Jags are specified with a mean and precision (1/variance), rather than with a mean and variance. So,  X ~ Normal(mean, variance) in JAGS would be translated as X~dnorm(mean,1/variance).
* The function dunif(a,b) indicates a uniform distribution with minimum value a and maximum value b.
* We **center** observations $y$ around the mean $\mu$ (mu) in the model. The centered variables are denoted $z$.
* We should read ``muy[t]`` as "the predictable part of y at time t"

```{r, eval=FALSE}
"
model{
   
####timepoint 1#######

	z[1] <- y[1]-mu  ###model for time point one.
	
#####model######
	for (t in 2:nt) ###model for timepoint 2 onwards
	{
		y[t]~dnorm(muy[t],Ipre)  ###Ipre is innovation (residual) precision
		muy[t]<- mu + phi*z[t-1] ## mu is mean, phi is autoregressive parameters
		z[t] <- y[t]-mu		##We do this step to center our observed variable in the model, so in the previous line we estimate a mean mu rather than an intercept there.
		
    }
	
			
####priors###########
       
    Ipre <- inverse(Ivar) ## innovation precision
    Ivar ~dunif(0,300)   ##innovation variance

    phi~dunif(-1,1) ##autoregression coefficient
    mu~dnorm(50,.001) ##mean
		
		      

}
"
```

We will fit the model in JAGS via package rjags with three chains. For each chain, initial values for each parameter can be specified in lists as follows:

```{r}
#specify initial values for each chain
ins = list(list( phi=.6, Ivar = 100,  mu=50),
           list( phi=.3, Ivar = 60,  mu=60),
           list( phi=-.2, Ivar = 80, mu = 70))

#specify the number of time points in the data (needed as data for the jags model, such that jags knows what 'nt' is in the JAGS code).
nt = length(dat_Jane)
```

It is also possible to not specify any starting values, in which case they will be generated automatically by jags - they will be sampled from the specified priors.

Next, we specify the model file, data to be used with the model file (for Jane), initial values, and the number of chains, for compiling the model in JAGS and rjags. We use a burn-in of 2000 samples per chain. We take 2000 samples on which we will base our results. 

```{r}

# Specify model file, data, initial values, and number of chains\
# compile and initialize jags AR model
jags_AR1 <- jags.model('AR1model.txt',
                   data = list('nt' = nt,
                               'y' = as.numeric(dat_Jane)
                   ),
                   inits=ins,
                   n.chains = 3) 

update(jags_AR1, 2000) # burn-in samples/iterations, to be discarded

# Take samples which we will use for the results
samps_AR1 = coda.samples(jags_AR1,
                         c('phi', 'mu', 'Ivar'),
                         2000)
```


### Checking Convergence
The model has been fitted. Before showing the results, we'll do some convergence checks, specifically, we check: 

  - trace plots, which should look like fat caterpillars, with good overlap (mixing) of the different chains (represented by different colors)
  - the Gelman-Rubin statistics (plots and summary stats), which should be very close to 1 (smaller than 1.05)
  - the density plots, which should look smooth and unimodel

```{r}
plot(samps_AR1,trace=FALSE) ##plot the marginal posterior densities, should look smooth, unimodal, usually normal

gelman.diag(samps_AR1) ###plot gelman rubin stats Should be 1 or very very near 1 (smaller than 1.05)

plot(samps_AR1,density=FALSE) ##traceplots should look like a pile of different colored hairy catterpillars
```

The density plots are nice and smooth, trace plots look like fat catterpillars, gelman-rubin statistics are good. Convergence seems in order.


### Interpreting parameter estimates
The resulting parameter estimates are given by:

```{r}
summary(samps_AR1) ##results parameter estimates
```

In the summary results you find the mean estimate for each parameter and the posterior standard deviation (similar in use to a frequentist standard error), and the quantiles for the posterior distribution. The mean or median estimate are often used as the point estimate for a parameter. The first and last quantile in the results can be used to form the lower and upper bound of a 95% credible interval for each parameter. 

Additional exercises:

* Are the results similar to those from arima and OLS procedure? 
* Rerun the model with a larger number of burnin and iterations. Do the results change (if the model has converged, the changes should be very small)? 
* Change the priors and starting values. Does it affect the results? Now fit the Bayesian AR(1) model for Mary.

##  Bayesian AR(2) analysis

Next, fit the AR2 model for Jane and Mary. You can try to adapt the AR(1) model code yourself, or use the pre-specified code in the file "AR2model.txt". If the latter, be sure to inspect this code and see if you can follow the alterations compared to the AR(1) model. Below we provide you with the code for specifying the starting values, running the model, checking convergence, and obatining parameter estaimtes

```{r, eval = F}
ins = list(list( phi=.6,phi2=.2, Ivar = 100,  mu=50),
           list( phi=.3,phi2=-.5, Ivar = 60,  mu=60),
           list( phi=-.2,phi2=0, Ivar = 80, mu = 70))  

jags_AR2 <- jags.model('AR2model.txt',
                   data = list('nt' = nt,
                               'y' = as.numeric(dat_Jane)
                               
                   ),
                   inits=ins,
                   n.chains = 3) 

update(jags_AR2, 3000) 

samps_AR2=coda.samples(jags_AR2,
                   c('phi','phi2', 'mu','Ivar'),
                   3000) 

plot(samps_AR2,trace=FALSE) 

gelman.diag(samps_AR2) 

plot(samps_AR2,density=FALSE) 

summary(samps_AR2) 


```

Has the model converged? If so, interpret the resulting parameter estimates. How do the results compare to the arima analysis?

## VAR(1) model in JAGS

As an alternative to the OLS approach in Exercise 3, we can fit a VAR(1) model in JAGs. Take a look at the JAGS model code for the bivariate VAR(1) model in `Var1model.txt', also shown below. The model is basically just a multivariate generalization of the AR(1) model. We have two lagged equations instead of just one, estimate two means, and four lagged regression coefficients, while instead of an innovation *variance*  we must supply a *variance-covariance matrix*. Take a look at the code provided below:  Do you understand the generalization of the AR(1) model to the multivariate model?





```{r, eval=FALSE}
"
model{
   

#####model######
	for (t in 2:nt) ###model for timepoint 2 onwards
	{
		y[t,1:2]~dmnorm(muy[t,1:2],Ipre[1:2,1:2])  ###Ipre is innovation precision
		muy[t,1]<- mu1 + phi11*z[t-1,1] + phi12*z[t-1,2] ## mu is mean, phi is autoregressive parameters
		muy[t,2]<- mu2 + phi22*z[t-1,2] + phi21*z[t-1,1] ## mu is mean, phi is autoregressive parameters
		z[t,1] <- y[t,1]-mu1	
		z[t,2] <- y[t,2]-mu2
    }

####timepoint 1#######

	z[1,1] <- y[1,1]-mu1  ###model for time point one variable 1.
    z[1,2] <- y[1,2]-mu2 ###model for time point one variable 2.
	
			
####priors###########
       
    Ipre <- inverse(Ivar) ## innovation precision matrix
    Ivar[1,1] ~dunif(0,300)   ##innovation variance 1
	Ivar[2,2] ~dunif(0,300)   ##innovation variance 2
	Ivar[1,2] <- Icor*sqrt(Ivar[1,1])*sqrt(Ivar[2,2])  ###innovation covariance
	Ivar[2,1] <- Ivar[1,2] ###innovation covariance
	Icor~dunif(-1,1) ###innovation correlation

    phi11~dnorm(0,.001) ##autoregression coefficient v1
	phi12~dnorm(0,.001) ##cross-lagged coefficient effect of v2 on v1
	phi21~dnorm(0,.001) ##cross-lagged coefficient effect of v1 on v2
	phi22~dnorm(0,.001) ##autoregression coefficient v2
	
    mu1~dnorm(50,.001) ##mean v1
    mu2~dnorm(3,.001) ##mean v2
		
		      

}
"

```

Fit the model for Jane. You should follow the same steps as outlined earlier, but there are simply more parameters to check the convergence of!

```{r, eval = F}
jags <- jags.model('VAR1model.txt',
                   data = list('nt' = nt,
                               'y' = datvar_Jane
                   ),
                   n.chains = 3) ###compile and initialize jags AR model

update(jags, 2000) ####burnin

samps=coda.samples(jags,
                   c('phi11','phi22', 'mu1','phi12','phi21', 'mu2', 'Ivar', 'Icor'),
                   3000) ### take samples next to burnin

plot(samps,trace=FALSE) ##plot densities

#gelman.diag(samps) ###plot gelman rubin stats

plot(samps,density=FALSE) ##traceplots

summary(samps) ##results parameters estimates 

```



## Bonus Exercises: Bayesian Extensions

Although these models aren't covered in class, I've prepared some examples of SEM-inspired ways in which time series models can be extended, which can be implemented easily when using a Bayesian approach. If you're primarily interested in how fitting multi-level time-series models works, skip ahead to the next exercise.

One of the nice features of using bayesian software to fit time series models is that it is relatively easy to extend your model in interesting ways. For example, one extension we might want to include is a **measurement model**, for example by including a **measurement error** term (see e.g., Schuurman, Houtveen & Hamaker, 2015; Schuurman & Hamaker, 2020). Dynamic errors (innovations) and measurement errors can be distinguished because the dynamic errors are carried over through the dynamic process, while the measurement errors are unique to a single time point. This means the model is only identified if the autoregressive effect is not exactly zero. It becomes harder to estimate the closer the autoregressive effect is to zero.

### Bonus 1: AR(1) with measurement error

The model AR(1) model with measurement error can be specified in JAGS as follows (see als`MEAR1model.txt'):

```{r, eval=FALSE}
"
model{
   

#####model######
	for (t in 2:nt)  ###model for timepoint 2 onwards
	{
		y[t]~dnorm(muy[t],Epre)  ###Epre is measurement precision
		muy[t]<- mu + ytilde[t]  ##mu is mean
		ytilde[t] ~ dnorm(muytilde[t], Ipre) ### Ipre is innovation precision
		muytilde[t] <- phi*ytilde[t-1]		 ##phi is autoregression coefficient
		
    }

####timepoint 1#######

	ytilde[1] <- y[1]-mu
    		
####priors###########
       
    Epre <- inverse(Evar) ##Measurement error precision
	Evar ~dunif(0,300)   ## measurement error variance
	Ipre <- inverse(Ivar) ## innovation precision
    Ivar ~dunif(0,300) ##innovation variance
	
	phi~dunif(-1,1) ##autoregression coefficient
	mu~dnorm(50,.001) ##mean
		
		      

}
"

```

We can fit the model for Jane using:

```{r, eval = F} 

ins = list(list( phi=.6, Ivar = 30, Evar = 20, mu=50),
           list( phi=.3, Ivar = 60, Evar = 40, mu=70),
           list( phi=-.2, Ivar = 20, Evar = 30, mu = 80))  ###initial values ARWN Jags

nt=length(scale)
jags <- jags.model('MEAR1model.txt',
                   data = list('nt' = length(as.numeric(mydata_Jane$scale)),
                               'y' = as.numeric(mydata_Jane$scale)
                   ),
                   inits=ins,
                   n.chains = 3) ###compile and initialize jags ARWN model

update(jags, 4000) ####burnin

samps=coda.samples(jags,
                   c('phi','mu','Ivar', 'Evar'),
                   4000) ###samples taken next to burnin


plot(samps,trace=FALSE) ###plot densities
#gelman.plot(samps) ##plot gelman rubin statistics
#gelman.diag(samps) ## table gelman rubin statistics
plot(samps,density=FALSE) ###traceplots

summary(samps) ##results parameter estimates

```

Be sure to check the convergence of the model; the MEAR model can be tricky to estimate, and needs plenty of iterations. If the model has not converged yet, increase the number of burning and samples to keep. 

### Bonus 2: Latent AR(1) model or Dynamic Factor Model
We can of course extend the previous approach, and model an AR(1) process on a latent variable Postive Affect, which is measured by several different observed variables. Both Jane and Mary provided daily evaluations of 8 positive affect items, namely "attentive", "strong", "interested", "enthusiastic", "proud", "determined", "inspired", and "exstatic". We will model a common positive affect factor for these items, and see if there is an autoregressive effect for latent Positive Affect. The model code can be found below, and in `DFAR1model.txt'.


```{r, eval=FALSE}
"model{
   

#####model######
	for (t in 2:nt) ###model for timepoint 2 onwards
	{
		y[t,1]~dnorm(muy[t,1],Ipre[1])  ###Ipre is residual/measurement precision
		muy[t,1]<- mu[1] + l[1]*f[t]
		y[t,2]~dnorm(muy[t,2],Ipre[2])  ###Ipre is residual/measurement precision
		muy[t,2]<- mu[2] + l[2]*f[t]
		y[t,3]~dnorm(muy[t,3],Ipre[3])  ###Ipre is residual/measurement precision
		muy[t,3]<- mu[3] + l[3]*f[t]
		y[t,4]~dnorm(muy[t,4],Ipre[4])  ###Ipre is residual/measurement  precision
		muy[t,4]<- mu[4] + l[4]*f[t]
		y[t,5]~dnorm(muy[t,5],Ipre[5])  ###Ipre is residual/measurement  precision
		muy[t,5]<- mu[5] + l[5]*f[t]
		y[t,6]~dnorm(muy[t,6],Ipre[6])  ###Ipre is residual/measurement  precision
		muy[t,6]<- mu[6] + l[6]*f[t]
		y[t,7]~dnorm(muy[t,7],Ipre[7])  ###Ipre is residual/measurement  precision
		muy[t,7]<- mu[7] + l[7]*f[t]
		y[t,8]~dnorm(muy[t,8],Ipre[8])  ###Ipre is residual/measurement  precision
		muy[t,8]<- mu[8] + l[8]*f[t]

		f[t]~dnorm(muf[t],Fpre)
		muf[t]<- phi*f[t-1]  #Note the factor mean is essentially fixed to zero by not including an intercept, for scaling purposes.
		
		
    }

   
	#####priors###########
    f[1]~dnorm(0,1) #model for time point 1 (should not matter too much what we specify here if we have enough repeated measures)
    
  	Fpre[1] <- inverse(Ivar[1]) ## innovation/dynamic error precision for the latent variable     Fvar[1] ~dunif(0,50)    ## innovation/dynamic error variance for the latent variable
    Ipre[1] <- inverse(Ivar[1]) 
    Ivar[1] ~dunif(0,50)   ##residual/measurement variance
	Ipre[2] <- inverse(Ivar[2])
    Ivar[2] ~dunif(0,50)   ##residual/measurement variance
	Ipre[3] <- inverse(Ivar[3]) 
    Ivar[3] ~dunif(0,50)  ###residual/measurement variance
	Ipre[4] <- inverse(Ivar[4]) 
    Ivar[4] ~dunif(0,50)   ##residual/measurement variance
	Ipre[5] <- inverse(Ivar[5]) 
    Ivar[5] ~dunif(0,50)   ##residual/measurement variance
	Ipre[6] <- inverse(Ivar[6]) 
    Ivar[6] ~dunif(0,50)  #residual/measurement variance
	Ipre[7] <- inverse(Ivar[7]) #
    Ivar[7] ~dunif(0,50)   ##residual/measurement variance
	Ipre[8] <- inverse(Ivar[8]) 
    Ivar[8] ~dunif(0,50)   ##residual/measurement variance


    phi~dunif(-1,1) ##autoregression coefficient
	mu[1]~dnorm(0,.001) ##mean
	mu[2]~dnorm(0,.001) ##mean
	mu[3]~dnorm(0,.001) ##mean
	mu[4]~dnorm(0,.001) ##mean
	mu[5]~dnorm(0,.001) ##mean
	mu[6]~dnorm(0,.001) ##mean
	mu[7]~dnorm(0,.001) ##mean
	mu[8]~dnorm(0,.001) ##mean
	l[1]<-1 #fixed to 1 for scaling purposes.
	l[2]~dnorm(0,.001) ##loading
	l[3]~dnorm(0,.001) ##loading
	l[4]~dnorm(0,.001)	##loading
	l[5]~dnorm(0,.001) ##loading
	l[6]~dnorm(0,.001) ##loading
	l[7]~dnorm(0,.001) ##loading
	l[8]~dnorm(0,.001) ##loading
		      

}
"
```


```{r, eval = F}
#Prepare the data
datser = data.frame(
  mydata_Jane$aandachtig,
  mydata_Jane$sterk,
  mydata_Jane$geinteresseerd,
  mydata_Jane$enthousiast,
  mydata_Jane$trots,
  mydata_Jane$vastberaden,
  mydata_Jane$geinspireerd,
  mydata_Jane$uitgelaten
)

datser = as.matrix(datser)

colnames(datser) = c("attentive", "strong", "interested", "enthusiastic", "proud", "determined", "inspired", "exstatic")  #english translations of the items we picked

ts.plot(datser[,1],col="mediumseagreen",lwd=3, ylab= "scores", ylim=c(1,6))
lines(datser[,2],col="pink",lwd=3)
lines(datser[,3],col="blue",lwd=3)
lines(datser[,4],col="green",lwd=3)
lines(datser[,5],col="orange",lwd=3)
lines(datser[,6],col="red",lwd=3)
lines(datser[,7],col="purple",lwd=3)
lines(datser[,8],col="yellow",lwd=3)


jags <- jags.model('DFAR1model.txt',
                   data = list('nt' = length(datser[,1]),
                               'y' = datser
                   ),
                   n.chains = 3) ###compile and initialize jags AR model

update(jags, 5000) ####burnin

samps=coda.samples(jags,
                   c('phi','l', 'mu','Ivar'),
                   10000) ### take samples next to burnin


plot(samps,trace=FALSE) ##plot densities

#gelman.plot(samps) ###plot gelman rubin stats
#gelman.diag(samps) ## table gelman rubin statistics

plot(samps,density=FALSE) ##traceplots

summary(samps) ##results parameters estimates 

```



# Ex.5: Bayesian Multilevel Time Series Models
Now that we have some practice fitting single-subject time series models using JAGS, it's time to see how we can extend this approach to the multilevel setting. Luckily, most of the tricky bits in specifying these models have already been dealt with in the single subject case. I think it's useful to keep in mind the "regularization" perspective when looking at the JAGS code: We want to fit a bunch of time-series models for each person, but we assume that the parameters of these models are drawn from a normal distribution!

Before getting started, there's one last file you'll need for this exercise, which is too large to share via github. You'll need this file to analyze the results of fitting the multilevel model (unless you have a few hours to wait around and fit it yourself!). You can download that file here: https://www.dropbox.com/s/7ebgoy5j95esnor/Samples_mlVAR1.Rdata?dl=

Let's first load the dataset we'll use in this part of the practical. The file `VAR1.dat` contains simulated time series data for 2 variables in the first two columns, for 200 individuals, and 70 repeated measures per individual (so 200*70 rows in the data file). Say that the 200 individuals are employees at a large company, and the two variables that were measured are the employees' experienced job motivation (Y1) and how much validation they experience from their employer (Y2). The third column in the dataset contains the participant numbers, the last two columns are lagged versions of variables Y1 and Y2. We will actually not use these lagged variables: We will have have the JAGS code specify lagged variables inside the model instead (best practice).

The data is stored in "long format", which means that the time series data of person 1 is placed in order on the first 70 rows, followed by the time series of participant 2 in row 71-140, etc. 

Take a look at the datafile to see how the data is ordered. Note that there are no missing observations in this simulated dataset.

We will now load the data in R with the following code, and store the dataset in object `mldata`:

```{r Results='hide', message=FALSE}
#load data
mldata = read.table("VAR1.dat", header = FALSE)
#give the variables names in R
colnames(mldata) <- c("Y1", "Y2", "ppn", "Y1lag1", "Y2lag1")

```


## Preliminary Set-Up
In order to run the JAGS models as I have set it up, we need to extract some information from the data. Specifically, we need to know at what row numbers in the dataset observations for participant $i$ start and end (i.e., what rows belong to what individuals). 


```{r,  message=FALSE}

# find all unique participant numbers
un = unique(mldata[, "ppn"]) 
# number of people in the dataset
np = length(un)
# how long is each individuals time-series?
nts <- sapply(un, function(i) length(which(mldata[,"ppn"] == i)))


# Get row numbers for start and end of each individuals time series
startseries = 0; endseries = 0
for (j in 1:length(un)) {
  startseries[j] = 1 + sum(nts[1:(j - 1)])
  endseries[j] = sum(nts[1:j])
}
startseries[1]=1
```

Based on this information, we can now also make plots of the time series of a number of participants. Take a look at the plots of the observed time series of a selection of participants, below. It's good practice to look at this for every individual to spot any differences between participants. Do you see many differences among the participants? 

```{r}
par(mfrow=c(2,2)) #tell R to put 6 plots on 1 page

for (j in 1:4) {
  # graph for variable 1 motivation
  ts.plot(mldata[(startseries[j]:endseries[j]),1], col="red", ylab = "y[t]") 
  # variable 2 validation 
  lines(mldata[(startseries[j]:endseries[j]),2], col="blue") 
}


```

The variables do not vary around the same value for each participant: this implies there may be differences in the means of the participants. Some participants seem to vary over a larger range of scores than other participants for these variables, which implies that their may be differences in the variability of the variables. This could be due to differences in dynamic parameters, or in residual variances. So, the participants seem to at least have some differences in their mean levels, and potentially their variances. It is not as easy to see directly from the plots if there may be differences in the participants' dynamics (such as autoregressive or cross-lagged effects).


## Fitting the multilevel VAR(1) model in JAGS
The JAGS code is supplied in the file `MLVAR1model.txt`. When we compare this to the single-subject VAR(1) model code we should start to see some similarities in how the code is structured. First off, we have the same basic equations as in the single subject case, with some additions. Here we have a "person" loop (`for j in 1:np`) as well as a "time" loop, as well as having some parameters with a person subscript (e.g. `b[j, 1]`). This denotes that those parameters are random effects, that is, they vary across people. 

Specifically, the parameters `b[j,1]` to `b[j,6]` each have a subscript j. These are person-specific means, autoregressive coefficients, and cross-lagged coefficients for each person. The innovation precision matrix Ipre, and hence also the covariance matrix Ivar, are not allowed to differ from person to person, so this parameter is 'fixed' across persons. 

For the parameters that are allowed to vary across persons we estimate the group averages (bmu parameters) and a precision and covariance matrix (bpre and bvar). We do this by specfying a prior for these parameters in JAGS.

### A note on prior specification
For the Bayesian model, we need to specify priors for all parameters, and today we aim to specify uninformative priors. For most of the parameters in the model it is not difficult to specify uninformative priors. You see that for the means of the person-specific means and regression coefficients, we use normal distributions with a very small precision (very large variance). Note again that in JAGS, normal distributions are specified with precisions (1/variance) rather than with variances. For the covariance matrix of the innovations, we use uniform (flat) distributions with a wide enough range based on what we know about our measured variables. 

However, specifying uninformative priors for the precision (inverse covariance) matrix of the random means, autoregressive effects and cross-lagged effects is more tricky! In Mplus, for example, the default priors for covariance matrices tend to work well for relatively simple multilevel VAR models like this one (but these may also become problematic when you have a very extensive model, such as multilevel VAR models with measurement error). The default priors that Mplus uses for the covariance matrix of the random effects are however not available in JAGS, so we have to solve this in a different way. For your practical today we have created a Wishart prior specification based on the method discussed in Schuurman, Grasman & Hamaker (2016) for the precision (inverse covariance) matrix of the random means, autoregressive effects and cross-lagged effects. This prior specification is stored in an R file and you will load this file later to give this prior specification to JAGS.  

More info on this specific issue (please feel free to skip):
We will specify a Wishart prior for the covariance matrix of the random effects, but the Wishart prior easily becomes very informative when variances are close to zero. Because our autoregressive effects are restricted in range (typically between 0 and 1 for psychological data) the variance for the coefficients will usually be close to zero. The variance for cross-lagged effects is also typically close to zero. So we have a problem with standard Wishart prior distributions (the same problems apply to gamma and chi2 distributions). To deal with this issue, we will specify a `data based' prior: we obtained a pre-estimate of the precision matrix for a simplified model, and use that to specify our prior. Although this means we will use the data twice, and this will result in slightly too small credible intervals for this covariance matrix, this performs better than various other priors (Schuurman, Grasman, Hamaker, 2016). Mplus v8 solves the problem in a different way, by specifying an "improper"" Wishart prior distribution (Improper priors are not available in JAGS). This solution works well for multilevel VAR models that are not very extensive, however problems may return with more complicated models, such as multivariate models with measurement error (see for example, Schuurman & Hamaker, 2020). It is always important to keep an eye on your priors. 

To obtain a pre-estimate for the variances of our covariance matrix for the random effects, we fitted a simplified Bayesian model with uniform priors on the variances of the random effects (ignoring the covariances between the random effects). We use those pre-estimates to specify a weakly informative Wishart prior. 

## Preparing starting values and data
Next, we will specify starting values for the model for JAGS. Here let's use 2 MCMC chains.

Note that we haven't specified starting values for the covariance matrix of the innovations. As a result, JAGS will randomly sample starting values for this covariance matrix from the priors that we specified. JAGS will do that for all parameters you do not provide starting values for.

```{r}
# starting values for the covariance matrix of the random effects
stbpre1 = matrix(c(.01, 0, 0, 0, 0, 0, 
                   0,.01,0, 0,0,0,
                   0,0, .01, 0 ,0,0,
                   0, 0, 0,0.01, 0, 0,
                   0, 0, 0, 0, .01, 0,
                   0, 0, 0, 0, 0, .01),6,6,byrow=TRUE)
stbpre2 = matrix(c(.015, 0, 0, 0, 0, 0,
                   0,.015,0, 0,0,0,
                   0,0, .015, 0 ,0,0,
                   0, 0, 0,0.015, 0, 0,
                   0, 0, 0, 0, .015, 0,
                   0, 0, 0, 0, 0, .015),6,6,byrow=TRUE)

# starting values for the fixed effects: the average person-specific coefficients.
bmustart1 = c(0, 0, 0, 0, 1, 1)
bmustart2 = c(0.2, 0.2, 0.2, 0.2, 1.5, 1.5)

# starting values for the individual-specific coefficients
phistart = matrix(0, np, 6)
phistart2 = phistart + .2


# Make one list that contains all the starting values we specified for each chain
inits1 <- list(bmu= bmustart1,  b=phistart, bpre=stbpre1) ###for chain1
inits2 <- list(bmu= bmustart2,  b=phistart2,  bpre=stbpre2) ###for chain 2

###Making one list that contains all the starting values for both chains. This is what we will supply to JAGS.
ins <- list(inits1,inits2) 



```

Next, we put all the data JAGS needs in one list, which we can supply that to JAGS later.

```{r}
###preparing the data for JAGS, we need the scores for Y1, Y2:
jagsdata=mldata[,1:2] ###jagsdata contains our raw data 

###Loading the 'scale' for the prior specification for the covariance matrix of the random effects
W = load("Wvar1.Rdata")
# If you want to see what W looks like, run 'W' in your R console.

# Now we make a list with all the data we need to feed to JAGS; the number of individuals np, the observed data jagsdata, our scale matrix that we prepared for the Wishart prior, and the row numbers of the start and end of each person's time series in the dataset.
dat = list(
  "np" = np,
  "y" = jagsdata,
  "W" = W,
  'startseries' = startseries,
  'endseries' = endseries
)

```

## Running the model

Running the model in JAGS is most likely (depending on the computing power of your laptop) going to take too long to be feasible for our practical session. Hence, instead of running the model in JAGS for real, we will load the results from the analysis we did previously. We do provide you with the necessary model code to run the model in JAGS. If you want, you can try this code in a separate R session.

```{r, eval = F}
###Tell JAGS about the model and the data

jags <- jags.model('MLVAR1model_Ex1.txt',
                  data = dat,
                   inits=ins,
                   n.chains = 2) ###compile and initialize jags AR model

##JAGS likes to do 'adaptation' before starting to estimate the parameters. It tries to make the estimation more efficient. We tell it to do adaptation for 10000 iterations and then stop adapting.
 adapt(jags,10000, end.adaptation=TRUE)

###Tell JAGS to take 30000 burn-in samples
update(jags, 30000)

###Tell JAGS to take another 30000 that we will base our parameter estimates on.

samps=coda.samples(jags,
                   c('bmu','bcov', 'Ivar','Icor', 'b'),
                   30000)

##Save the resulting Bayesian iterations (without burn-in) for each parameter
save(samps,file= "Samples_mlVAR1.Rdata")
```

As you can see from the above code, we stored the samples from the Bayesian estimation procedure for the parameters bmu, bcov, Ivar, Icor, and b (b refers to b[1,j] to b[6,j], for every person) in object 'samps', and saved this object in file "Samples_mlVAR1.Rdata". We will use that object to complete the rest of the exercises. 

```{r}

load(file="Samples_mlVAR1.Rdata")

#below we separately store the group level results for ease of viewing later. In it are the average person-specific parameters (bmu), and all elements of the covariance (bcov) and correlation (bcor) matrix of the person-specific parameters.

group_level_samps= samps[, c('bmu[1]','bmu[2]','bmu[3]','bmu[4]','bmu[5]','bmu[6]','Ivar[1,1]','Ivar[2,2]','Ivar[1,2]','Icor','bcov[1,1]','bcov[2,2]','bcov[3,3]','bcov[4,4]','bcov[5,5]','bcov[6,6]',
'bcor[1,2]','bcor[1,3]','bcor[1,4]','bcor[1,5]','bcor[1,6]','bcor[2,3]','bcor[2,4]','bcor[2,5]','bcor[2,6]','bcor[3,4]', 'bcor[3,5]','bcor[3,6]','bcor[4,5]','bcor[4,6]','bcor[5,6]')]


```


## Check Convergence
As we did in the single subject case, before interpreting the model parameters, we first check convergence of the model. Note that checking all of the parameters estimated in the model one-by-one may not be feasible, since we estimate 6 parameters for each of the 200 individuals in the dataset. Instead, let's focus on the "group-level" parameters, that is, the average intercepts, auto-regressive and cross-lagged effects (also referred to as the "fixed effects"). of course, if you are interested in specific persons, then you should check convergence for those person-specific parameters as well.


```{r}
plot(group_level_samps,trace=FALSE) ##plot (approximated) posterior densities

gelman.diag(group_level_samps) ###table gelman rubin stats
#gelman.plot(group_level_samps) ###plot gelman rubin stats

plot(group_level_samps,density=FALSE) ##traceplots
```


This is a fairly complicated model, and you see that for a number of parameters (particularly the covariance matrix of the random effects) convergence seems a bit less tidy than what you have seen yesterday. Particularly, the Bayesian samples are more strongly autocorrelated. This means that we will need a lot of samples to get a good picture of the posterior. Another thing that makes convergence for this matrix a bit harder is that the variance of one of the cross-lagged effects is very very very small; less than 0.001. The chains are mixing however, and ending up at the same place, with approximately the same width, so that is good. The density plots are nice and smooth. The gelman rubin stats for some of the parameters could be better. So what one would do in this case, is run the model with more chains with different starting values, and for an even longer set of iterations, to see if the results are substantially different. 
    
Given that this is a simulated dataset, we know the true values, and we can tell you that the model has actually converged ok, and we will continue with interpreting the model! 

## Interpreting the results

Now look at the group level results. This contains the (co)variances of the innovations (assumed to be the same for every person and the group overall), the (co)variances between the random parameters, the average random parameters. 

```{r}
#summary(samps) # this line of code would provide your with results for all parameters estimates, including the person-specific parameters. 

#check out the group level results
group_results=summary(group_level_samps) 
group_results


```


We see that on average, there is a positive autoregression coefficient for both variables. The autoregressive effect for job motivation is larger than for job validation. We see evidence for a positive cross-lagged effect of job validation on job motivation, on average across persons. On average, employees that experience validation at one measurement occassion, tend to feel more motivated at the next occasion. There is no evidence for an effect of job motivation on job validation, on average.


Icor is the correlation between the innovations. Ivar[1,1] and Ivar[2,2] are the innovation variances for each variable. There is evidence for a positive correlation between the innovations of variable Y1 and Y2. That indicates that after controlling for the autoregressive and crosslagged effects, there remains a concurrent/contemporaneous correlation between the two variables. This relationship is positive, which implies that when a persons job validation is high at a certain time point, their motivation tends to also be high at that same time point, after controlling for the lagged effects.

## Further Analyses

Above we looked at the fixed effects, but we didn't really get much of a picture of the degree to which these parameter values vary across individuals. To get an insight into that, we need to look at the distribution of the person-specific parameter estimates. 

First, we obtain a point-estimate for each person-specific parameter using the median of their posterior.

```{r}
all_results <- summary(samps) 

## Person-specific parameters (median estimates) per type of parameter:
random_mean_1 = all_results[[2]][6:205, 3]
random_mean_2 = all_results[[2]][206:405, 3]
random_cross_12 = all_results[[2]][406:605, 3]
random_cross_21 = all_results[[2]][606:805, 3]
random_auto_1 = all_results[[2]][806:1005, 3]
random_auto_2 = all_results[[2]][1006:1205, 3]
```

Now, we can look at the distribution of the person-specific point estimates to get an idea of the degree to which people differ from the average parameter (intercept, auto- and cross-lagged effect)

```{r}
par(mfrow = c(3,2))
#histograms of the person specific parameters (median estimates per person)
hist(random_mean_1, main="Person-specific Means 1 Mot",xlab="Person-spec. Means 1 Mot")
hist(random_mean_2, main="Person-specific Means 2 Val",xlab="Person-spec. Means 2 Val")
hist(random_cross_12, main="Person-specific Cross-lagged Effects 12 Val->Mot",xlab="Person-spec. Cross-l. 12")
hist(random_cross_21, main="Person-specific Cross-lagged Effects 21 Mot->Val",xlab="Person-spec. Cross-l. 21")
hist(random_auto_1, main="Person-specific Autoregressive Effects 1 Mot",xlab="Person-spec. Auto. 1 Mot")
hist(random_auto_2, main="Person-specific Autoregressive Effects 2 Val",xlab="Person-spec. Auto. 2 Val")
```

There is notable variance around the means, but also around the regression coefficients. 

The variance for the random autoregressive and cross-lagged effects seem small in absolute terms, but they are not in relative terms: Remember that the parameters themselves also have a fairly small range (approximately -1 to 1), so a small variance can already make a big difference in what parameter values are plausible. If you make a distribution of the random parameters, as you can see from the histograms, you see that the autoregressive coefficients and cross-lagged coefficient 12 (effect of validation on motivation) actually differ quite a bit from person to person. There is relatively little variance around the effect of job motivation on job validation; these coefficients range from -.03 to .03 and hence are quite similar from person to person.

If you look at the person-specific results for this parameter, including the credible intervals for each person with `all_results[[2]][606:805,]` you will furthermore see that for each participant the credible intervals contain zero, with lower and upper bounds roughly around -0.05 to 0.05. Hence, we have no evidence that day to day motivation affects the day to day validation for these persons.

We can also examine the covariances between the random effects using:
```{r}
plot(random_mean_1, random_mean_2,xlab="Person-specific Means 1 Mot",ylab="Person-specific Means 2 Val" )
plot(random_auto_2,random_cross_12, xlab="Person-specific Autoregressive Effects 1 Val",ylab="Person-specific Cross-lagged Effects 12 Val->Mot")
```

There is evidence for a positive association between the person-specific means; People with higher means on job motivation tend to also have higher means for job validation. There is evidence for a positive association between the autoregressive effect of job validation, and the cross-lagged effect of job validation on job motivation. People that have a high autoregressive effect for job validation tend to have a higher cross-lagged effect of job validation on job motivation.

Finally, you can inspect the results of any individual participant using the following code (for individual 15)

```{r, eval = F}
summary(samps)$statistics[c('b[15,1]','b[15,2]','b[15,3]','b[15,4]','b[15,5]','b[15,6]'),]
summary(samps)$quantiles[c('b[15,1]','b[15,2]','b[15,3]','b[15,4]','b[15,5]','b[15,6]'),]
```


# End

This is the end of our practical on discrete-time time series models in R. We've walked you through the basics of specifying single-subject and multi-level uni-variate and multi-variate time series models, using basic OLS approaches and bayesian modeling in JAGS. Note that there are many other programs in which you can fit these models, though generally, Bayesian modeling is preferred for multi-level time series models. For more on how to specify these same models in other programs, such as `stan` and `Mplus`, you can check out this recent paper:

Li, Y., Wood, J., Ji, L., Chow, S. M., & Oravecz, Z. (2021). Fitting Multilevel Vector Autoregressive Models in Stan, JAGS, and Mplus. Structural Equation Modeling: A Multidisciplinary Journal, 1-24.
